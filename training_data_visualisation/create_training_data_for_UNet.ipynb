{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating TFRecord files for UNet-based segmentation\n",
    "\n",
    "U-Net: Convolutional Networks for Biomedical Image Segmentation  \n",
    "Olaf Ronneberger, Philipp Fischer and Thomas Brox  \n",
    "http://arxiv.org/abs/1505.04597\n",
    "\n",
    "\n",
    "### DATA ORGANISATION\n",
    "\n",
    "The image list is used like this:\n",
    "\n",
    "```json\n",
    "images = ['phase','rfp']\n",
    "\n",
    "set1/\n",
    "  phase/\n",
    "    \n",
    "  gfp/\n",
    "    0001_gfp.tif\n",
    "    0002_gfp.tif\n",
    "    ...\n",
    "  rfp/\n",
    "    0001_rfp.tif\n",
    "    0002_rfp.tif\n",
    "  ...\n",
    "  labels/\n",
    "    0001_mask.tif\n",
    "    0002_mask.tif\n",
    "    ...\n",
    "  weights/\n",
    "    0001_weights.tif  # NOTE(arl): these are calculated\n",
    "    0002_weights.tif\n",
    "    ...\n",
    "set2/\n",
    "```\n",
    "\n",
    "### STEPS\n",
    "1. Calculate the weightmaps for the images \n",
    "2. Create the TFRecord file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data filename\n",
    "DATA_PATH = '/home/nathan/data/training/training_data_original/'\n",
    "# NOTE TO SELF (NAT) THINK THE OLD WEIGHT FILES+DIRs NEED TO BE DELETED BEFORE NEW ONES ARE MADE\n",
    "#DATA_PATH = \"/home/nathan/analysis/training/training_data\"\n",
    "WEIGHT_AMPLITUDE = 50.\n",
    "tfrecord_fn = \"SHARC_UNet_v3_w0-\"+str(WEIGHT_AMPLITUDE)+\".tfrecord\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import enum\n",
    "import numpy as np\n",
    "from skimage import io\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions to find files and folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@enum.unique\n",
    "class Channels(enum.Enum):\n",
    "    BRIGHTFIELD = 0 \n",
    "    GFP = 1\n",
    "    RFP = 2\n",
    "    IRFP = 3\n",
    "    PHASE = 4\n",
    "    WEIGHTS = 98\n",
    "    MASK = 99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_metadata():\n",
    "    jfile = os.path.join(DATA_PATH, 'training_metadata.json')\n",
    "    with open(jfile, 'r') as json_file:\n",
    "        metadata = json.load(json_file)\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "metadata = read_metadata()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function to write out the TFRecord file used by the server to train the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_tfrecord(pth,\n",
    "                   filename,\n",
    "                   metadata,\n",
    "                   weights_dir_base='weights'):\n",
    "    \n",
    "    try:\n",
    "        import tensorflow as tf\n",
    "    except ImportError:\n",
    "        raise ImportError(\"Tensorflow is not installed.\")\n",
    "        \n",
    "    # NOTE(arl): set the number of output channels as n_input+1, this is a guess though    \n",
    "    # parse the metadata, set the number of output channels by guessing\n",
    "    channels = [Channels[c.upper()] for c in metadata.keys() if c not in ('mask', 'weights')]\n",
    "    print(channels)\n",
    "    num_outputs = 2 # len(channels)+1 # only works for multidimensional masks\n",
    "    \n",
    "    # _int64 is used for numeric values\n",
    "    def _int64_feature(value):\n",
    "        return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "    # _bytes is used for string/char values\n",
    "    def _bytes_feature(value):\n",
    "        return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "    # _floats is used for float values\n",
    "    def _float_feature(value):\n",
    "        return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "    def convert_to_mask(truth):\n",
    "        if truth.ndim == 2:\n",
    "            unique_labels = [l for l in np.unique(truth).tolist() if l>0]\n",
    "            print(f'Unique labels in mask: {unique_labels}')\n",
    "            mask = np.zeros(truth.shape, dtype=np.uint8)\n",
    "            for i, label in enumerate(unique_labels):\n",
    "                mask[truth==label] = i+1\n",
    "            return mask\n",
    "        \n",
    "        # otherwise we have a multidimensional mask\n",
    "        mask = np.zeros(truth.shape[1:], dtype=np.uint8)\n",
    "        unique_labels = [i+1 for i in range(truth.shape[0])]\n",
    "        print(f'Unique labels in mask: {unique_labels}')\n",
    "        for i, label in enumerate(unique_labels):\n",
    "            mask[truth[i,...]>0] = label\n",
    "            mask[truth[i,...]>0] = label\n",
    "        return mask\n",
    "    \n",
    "    # set up the writer\n",
    "    #writer = tf.python_io.TFRecordWriter(os.path.join(pth, filename))\n",
    "    writer = tf.io.TFRecordWriter(os.path.join(pth, filename))\n",
    "\n",
    "\n",
    "    for i in range(len(metadata['mask'])):\n",
    "        \n",
    "        # get the image data, remove any singleton dimensions \n",
    "        print(i)\n",
    "        for c in channels:\n",
    "            print(c.name.lower())\n",
    "\n",
    "            #i_data = np.stack([io.imread(os.path.join(DATA_PATH, metadata[c.name.lower()[i]]))], axis=-1)\n",
    "            i_data = np.stack([io.imread(os.path.join(DATA_PATH, metadata[c.name.lower()][i])) for c in channels], axis=-1)\n",
    "        \n",
    "        # get the label data \n",
    "        l_data = convert_to_mask(io.imread(os.path.join(DATA_PATH, metadata['mask'][i])))\n",
    "        \n",
    "        plt.figure()\n",
    "        plt.imshow(l_data)\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "\n",
    "        # get the weights data\n",
    "        w_data = io.imread(os.path.join(DATA_PATH, metadata['weights'][i])).astype(np.float32)\n",
    "\n",
    "        print(f'Input: {str((i_data.shape, i_data.dtype))}')\n",
    "        print(f'Output: {str((l_data.shape[0], l_data.shape[1], num_outputs))}')\n",
    "\n",
    "        # set up TF feature dict\n",
    "        feature = {'train/image/image': _bytes_feature(i_data.tostring()),\n",
    "                   'train/image/width': _int64_feature(i_data.shape[1]),\n",
    "                   'train/image/height': _int64_feature(i_data.shape[0]),\n",
    "                   'train/image/depth': _int64_feature(i_data.shape[-1]),\n",
    "                   'train/label/image': _bytes_feature(l_data.tostring()),\n",
    "                   'train/label/width': _int64_feature(l_data.shape[1]),\n",
    "                   'train/label/height': _int64_feature(l_data.shape[0]),\n",
    "                   'train/label/depth': _int64_feature(num_outputs),\n",
    "                   'train/weight/image': _bytes_feature(w_data.tostring()),\n",
    "                   'train/weight/width': _int64_feature(w_data.shape[1]),\n",
    "                   'train/weight/height': _int64_feature(w_data.shape[0]),\n",
    "                   'train/weight/depth': _int64_feature(1)}\n",
    "\n",
    "        features = tf.train.Features(feature=feature)\n",
    "        example = tf.train.Example(features=features)\n",
    "\n",
    "        # write out the serialized features\n",
    "        writer.write(example.SerializeToString())\n",
    "\n",
    "    # close up shop\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Write out the TFRecord file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Tensorflow is not installed.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-ea94d77dcb4b>\u001b[0m in \u001b[0;36mwrite_tfrecord\u001b[0;34m(pth, filename, metadata, weights_dir_base)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-826204384d9f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwrite_tfrecord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtfrecord_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"I AM DONE THANK YOU VERY MUCH\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-ea94d77dcb4b>\u001b[0m in \u001b[0;36mwrite_tfrecord\u001b[0;34m(pth, filename, metadata, weights_dir_base)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tensorflow is not installed.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# NOTE(arl): set the number of output channels as n_input+1, this is a guess though\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: Tensorflow is not installed."
     ]
    }
   ],
   "source": [
    "write_tfrecord(DATA_PATH, tfrecord_fn, metadata)\n",
    "print(\"I AM DONE THANK YOU VERY MUCH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
